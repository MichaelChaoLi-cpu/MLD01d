{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "These are experimental codes:      \n",
    "- Briefing the data\n",
    "- Tuning optimal hyperparameter\n",
    "- Visualize the performance of Models\n",
    "- Visualize the importance\n",
    "- Visualize and summerize impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Modelling\n",
    "import SettingForFeatures\n",
    "import TestingTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(SettingForFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_aim(\n",
    "    all_data : pd.DataFrame,\n",
    "    aim_variable: str,\n",
    "    always_inputs: list,\n",
    "    reg_params : dict,\n",
    "    n_splits : int = 10\n",
    "): \n",
    "    all_data_use =  all_data[\n",
    "        always_inputs + [aim_variable]\n",
    "    ]\n",
    "    X = all_data_use.drop(columns = aim_variable)\n",
    "    y = all_data_use[aim_variable]\n",
    "\n",
    "    _ = Modelling.xgb_cls_kfold_cv(\n",
    "        X, y,\n",
    "        n_splits=n_splits,\n",
    "        params=reg_params,\n",
    "        log_dir=\"logs\",\n",
    "        log_file=\"xgb_cls_booming.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_params = {\n",
    "    \"objective\":\"binary:logistic\",  \n",
    "    \"eval_metric\":\"logloss\",   \n",
    "    \"n_estimators\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(os.getenv(\"PROJECT_ROOT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "always_inputs = SettingForFeatures.return_input_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_output = SettingForFeatures.return_output_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = SettingForFeatures.data_load_combine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAUTIFUL_NAME = SettingForFeatures.return_beautiful_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aim_variable in potential_output:\n",
    "    print(aim_variable)\n",
    "    random_search_aim(\n",
    "        all_data = df_all,\n",
    "        aim_variable = aim_variable,\n",
    "        always_inputs = always_inputs,\n",
    "        reg_params = cls_params,\n",
    "        n_splits = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Basic Importance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aim_variable in potential_output:\n",
    "    X, y = Modelling.prepare_data(\n",
    "        all_data = df_all,\n",
    "        always_inputs = always_inputs,\n",
    "        aim_variable = aim_variable,\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    feature_importance_df_list = []\n",
    "    fold = 1\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "    \n",
    "        # Split\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        # Train\n",
    "        model = xgb.XGBClassifier(**cls_params)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Metrics\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "        print(f\"Fold {fold}: accuracy={accuracy:.4f}, precision={precision:.4f}, recall={recall:.4f}\")\n",
    "    \n",
    "        # Importance\n",
    "        booster = model.get_booster()\n",
    "        score = booster.get_score(importance_type='gain')\n",
    "    \n",
    "        # Convert to aligned DF: feature as index, importance as column\n",
    "        df_imp = pd.DataFrame(score, index=[0]).T\n",
    "        df_imp.columns = [f\"fold_{fold}\"]\n",
    "    \n",
    "        feature_importance_df_list.append(df_imp)\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    # ðŸ”¥ Merge all folds by feature name\n",
    "    feature_importance_full = pd.concat(feature_importance_df_list, axis=1).fillna(0)\n",
    "    \n",
    "    feature_importance_full['mean_importance'] = feature_importance_full.mean(axis = 1)\n",
    "    feature_importance_full = feature_importance_full.sort_values('mean_importance')\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_full.index, feature_importance_full[\"mean_importance\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Gain Importance\")\n",
    "    plt.title(\"XGBoost Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    importance_list.append(feature_importance_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS := 'results', exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, aim_variable in enumerate(potential_output):\n",
    "    importance_list[idx].index = importance_list[idx].index.map(\n",
    "        lambda x: BEAUTIFUL_NAME.get(x, x)\n",
    "    )\n",
    "    importance_list[idx].to_excel(os.path.join(RESULTS, f'importnace_basic_{aim_variable}.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
