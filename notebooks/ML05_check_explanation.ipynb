{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Check Explanation of the relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import yaml\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ExplainResult\n",
    "import Modelling\n",
    "import SettingForFeatures\n",
    "import TestingTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(Modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serializable(obj):\n",
    "    if hasattr(obj, 'item'):\n",
    "        return obj.item()\n",
    "    if hasattr(obj, 'tolist'):\n",
    "        return obj.tolist()\n",
    "    raise TypeError(f\"Type not serializable: {type(obj)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_pdp_self_defined(\n",
    "    var: str,\n",
    "    X: pd.DataFrame,\n",
    "    model_list: list,\n",
    "    range_boundary=(0.05, 0.95),\n",
    "    stripe: float = 0.2\n",
    ") -> tuple:\n",
    "    # Determine the grid boundaries based on the specified quantiles\n",
    "    low_b = range_boundary[0]\n",
    "    up_b = range_boundary[1]\n",
    "    \n",
    "    # Generate the discrete grid of feature values\n",
    "    potenital_values = np.arange(low_b, up_b, stripe)\n",
    "    X_adjust = X.copy()\n",
    "    \n",
    "    pdp_list = []\n",
    "    \n",
    "    # Iterate through each model in the ensemble/list\n",
    "    for model in model_list:\n",
    "        pdp = np.full_like(potenital_values, fill_value=np.nan)\n",
    "        \n",
    "        # Iterate through each grid point (potential value)\n",
    "        for idx, potenital_value in enumerate(potenital_values):\n",
    "            # 1. Substitute the feature column with the current fixed value\n",
    "            X_adjust[var] = potenital_value.astype(float)\n",
    "            \n",
    "            # 2. Predict the outcome for the entire adjusted dataset\n",
    "            y_pred = model.predict_proba(X_adjust)[:,1]\n",
    "            \n",
    "            # 3. Calculate the partial dependence (average prediction)\n",
    "            pdp[idx] = np.mean(y_pred)\n",
    "        \n",
    "        pdp_list.append(pdp)\n",
    "\n",
    "    pdp_array = np.array(pdp_list)\n",
    "\n",
    "    return potenital_values, pdp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(os.getenv(\"PROJECT_ROOT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = SettingForFeatures.data_load_combine_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "always_inputs = SettingForFeatures.return_input_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_variables = SettingForFeatures.return_output_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aim_variable in aim_variables:\n",
    "    X, y = Modelling.prepare_data(\n",
    "        all_data = all_data,\n",
    "        always_inputs = always_inputs,\n",
    "        aim_variable = aim_variable,\n",
    "    )\n",
    "\n",
    "    with open(f\"./{aim_variable}_params.yaml\", \"r\") as f:\n",
    "        params = yaml.safe_load(f)\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    r2_list = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Split\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        # Train\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    \n",
    "        print(accuracy, f1, precision, recall)\n",
    "        r2_list.append(accuracy)\n",
    "\n",
    "    r2_ols_list = []\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Split\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        # Train\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    \n",
    "        print(accuracy, f1, precision, recall)\n",
    "        r2_ols_list.append(accuracy)\n",
    "\n",
    "    accuracy_comparison = params\n",
    "    accuracy_comparison['XGBoost Mean Accuracy'] = np.mean(r2_list).astype(float)\n",
    "    accuracy_comparison['XGBoost SD Accuracy'] = np.std(r2_list).astype(float)\n",
    "    accuracy_comparison['XGBoost Min Accuracy'] = np.min(r2_list).astype(float)\n",
    "    accuracy_comparison['XGBoost Max Accuracy'] = np.max(r2_list).astype(float)\n",
    "\n",
    "    accuracy_comparison['Logistic Mean Accuracy'] = np.mean(r2_ols_list).astype(float)\n",
    "    accuracy_comparison['Logistic SD Accuracy'] = np.std(r2_ols_list).astype(float)\n",
    "    accuracy_comparison['Logistic Min Accuracy'] = np.min(r2_ols_list).astype(float)\n",
    "    accuracy_comparison['Logistic Max Accuracy'] = np.max(r2_ols_list).astype(float)\n",
    "\n",
    "    save_path = os.path.join('results', f'{aim_variable}_accuracy_comparison.json')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(\n",
    "            accuracy_comparison,\n",
    "            f,\n",
    "            default=json_serializable,\n",
    "            ensure_ascii=False,\n",
    "            indent=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Check Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aim_variable in aim_variables:\n",
    "    X, y = Modelling.prepare_data(\n",
    "        all_data = all_data,\n",
    "        always_inputs = always_inputs,\n",
    "        aim_variable = aim_variable,\n",
    "    )\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    with open(f\"./{aim_variable}_params.yaml\", \"r\") as f:\n",
    "        params = yaml.safe_load(f)\n",
    "    \n",
    "    print(params)\n",
    "\n",
    "    feature_importance_df_list = []\n",
    "    fold = 1\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "    \n",
    "        # Split\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        # Train\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "        # Metrics\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "        print(f\"Fold {fold}: accuracy={accuracy:.4f}, precision={precision:.4f}, recall={recall:.4f}\")\n",
    "    \n",
    "        # Importance\n",
    "        booster = model.get_booster()\n",
    "        score = booster.get_score(importance_type='gain')\n",
    "    \n",
    "        # Convert to aligned DF: feature as index, importance as column\n",
    "        df_imp = pd.DataFrame(score, index=[0]).T\n",
    "        df_imp.columns = [f\"fold_{fold}\"]\n",
    "    \n",
    "        feature_importance_df_list.append(df_imp)\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    # ðŸ”¥ Merge all folds by feature name\n",
    "    feature_importance_full = pd.concat(feature_importance_df_list, axis=1).fillna(0)\n",
    "    \n",
    "    feature_importance_full['mean_importance'] = feature_importance_full.mean(axis = 1)\n",
    "    feature_importance_full = feature_importance_full.sort_values('mean_importance')\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.barh(feature_importance_full.index, feature_importance_full[\"mean_importance\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Gain Importance\")\n",
    "    plt.title(\"XGBoost Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    feature_importance_full.to_csv(os.path.join('results', f'{aim_variable}_importance.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Check PDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Health indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for aim_variable in aim_variables:\n",
    "    print(aim_variable)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./{aim_variable}_params.yaml\", \"r\") as f:\n",
    "    params = yaml.safe_load(f)\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_variable = aim_variables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Modelling.prepare_data(\n",
    "    all_data = all_data,\n",
    "    always_inputs = always_inputs,\n",
    "    aim_variable = aim_variable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = Modelling.get_clsmodel_list(\n",
    "    X, y,\n",
    "    n_splits = n_splits,\n",
    "    params = params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['DisasterExpInd'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "potenital_values, pdp_array = compute_single_pdp_self_defined(\n",
    "    var = 'DisasterExpInd',\n",
    "    X = X,\n",
    "    model_list = model_list,\n",
    "    range_boundary = (0.0, 11.0),\n",
    "    stripe = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_mean = np.mean(pdp_array, axis = 0)\n",
    "pdp_std = np.std(pdp_array, axis = 0)\n",
    "\n",
    "# 1. Plot the mean PDP line (same as before)\n",
    "plt.plot(potenital_values, pdp_mean, linewidth=2, label=\"Mean Prediction\")\n",
    "\n",
    "# 2. Add the Error/Confidence Band using fill_between\n",
    "# The band represents [mean - std] to [mean + std]\n",
    "plt.fill_between(\n",
    "    potenital_values, \n",
    "    pdp_mean - pdp_std * 1.96,  # Lower bound\n",
    "    pdp_mean + pdp_std * 1.96,  # Upper bound\n",
    "    color='gray',        # Color of the shaded area\n",
    "    alpha=0.3,           # Transparency\n",
    "    label=\"$\\pm 1.96 \\sigma$\" # Label for the legend\n",
    ")\n",
    "\n",
    "# Optional: Add labels and grid\n",
    "plt.xlabel(\"Natural Disaster Count\")\n",
    "plt.ylabel(\"Predicted Disease Increase Probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(os.path.join(FIGURES, f'fig06_{aim_variable}_PDP_{varname.upper()}_STD.jpg'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Naive PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the grid boundaries based on the specified quantiles\n",
    "low_b = 0.0\n",
    "up_b = 11.0\n",
    "stripe = 1.0\n",
    "var = 'DisasterExpInd'\n",
    "\n",
    "# Generate the discrete grid of feature values\n",
    "potenital_values = np.arange(low_b, up_b, stripe)\n",
    "X_adjust = X.copy()\n",
    "\n",
    "pdp_list = []\n",
    "\n",
    "# Iterate through each model in the ensemble/list\n",
    "for model in model_list:\n",
    "    pdp = np.full_like(potenital_values, fill_value=np.nan)\n",
    "    \n",
    "    # Iterate through each grid point (potential value)\n",
    "    for idx, potenital_value in enumerate(potenital_values):\n",
    "        # 1. Substitute the feature column with the current fixed value\n",
    "        X_adjust[var] = potenital_value.astype(float)\n",
    "        \n",
    "        # 2. Predict the outcome for the entire adjusted dataset\n",
    "        y_pred = model.predict_proba(X_adjust)[:,1]\n",
    "        \n",
    "        # 3. Calculate the partial dependence (average prediction)\n",
    "        pdp[idx] = np.mean(y_pred)\n",
    "    \n",
    "    pdp_list.append(pdp)\n",
    "\n",
    "pdp_array = np.array(pdp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_mean = np.mean(pdp_array, axis = 0)\n",
    "pdp_std = np.std(pdp_array, axis = 0)\n",
    "\n",
    "# 1. Plot the mean PDP line (same as before)\n",
    "plt.plot(potenital_values, pdp_mean, linewidth=2, label=\"Mean Prediction\")\n",
    "\n",
    "# 2. Add the Error/Confidence Band using fill_between\n",
    "# The band represents [mean - std] to [mean + std]\n",
    "plt.fill_between(\n",
    "    potenital_values, \n",
    "    pdp_mean - pdp_std * 1.96,  # Lower bound\n",
    "    pdp_mean + pdp_std * 1.96,  # Upper bound\n",
    "    color='gray',        # Color of the shaded area\n",
    "    alpha=0.3,           # Transparency\n",
    "    label=\"$\\pm 1.96 \\sigma$\" # Label for the legend\n",
    ")\n",
    "\n",
    "# Optional: Add labels and grid\n",
    "plt.xlabel(\"Natural Disaster Count\")\n",
    "plt.ylabel(\"Predicted Disease Increase Probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES, f'fig05_naive_PDP.jpg'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### without know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the grid boundaries based on the specified quantiles\n",
    "low_b = 0.0\n",
    "up_b = 11.0\n",
    "stripe = 1.0\n",
    "var = 'DisasterExpInd'\n",
    "\n",
    "# Generate the discrete grid of feature values\n",
    "potenital_values = np.arange(low_b, up_b, stripe)\n",
    "X_adjust = X.copy()\n",
    "\n",
    "pdp_list = []\n",
    "\n",
    "# Iterate through each model in the ensemble/list\n",
    "for model in model_list:\n",
    "    pdp = np.full_like(potenital_values, fill_value=np.nan)\n",
    "    \n",
    "    # Iterate through each grid point (potential value)\n",
    "    for idx, potenital_value in enumerate(potenital_values):\n",
    "        # 1. Substitute the feature column with the current fixed value\n",
    "        X_adjust[var] = potenital_value.astype(float)\n",
    "        X_adjust['HeardClimate_Dummy'] = 0.0\n",
    "        \n",
    "        # 2. Predict the outcome for the entire adjusted dataset\n",
    "        y_pred = model.predict_proba(X_adjust)[:,1]\n",
    "        \n",
    "        # 3. Calculate the partial dependence (average prediction)\n",
    "        pdp[idx] = np.mean(y_pred)\n",
    "    \n",
    "    pdp_list.append(pdp)\n",
    "\n",
    "pdp_array = np.array(pdp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_array_without_knowledge = pdp_array.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_mean = np.mean(pdp_array, axis = 0)\n",
    "pdp_std = np.std(pdp_array, axis = 0)\n",
    "\n",
    "# 1. Plot the mean PDP line (same as before)\n",
    "plt.plot(potenital_values, pdp_mean, linewidth=2, label=\"Mean Prediction\")\n",
    "\n",
    "# 2. Add the Error/Confidence Band using fill_between\n",
    "# The band represents [mean - std] to [mean + std]\n",
    "plt.fill_between(\n",
    "    potenital_values, \n",
    "    pdp_mean - pdp_std * 1.96,  # Lower bound\n",
    "    pdp_mean + pdp_std * 1.96,  # Upper bound\n",
    "    color='gray',        # Color of the shaded area\n",
    "    alpha=0.3,           # Transparency\n",
    "    label=\"$\\pm 1.96 \\sigma$\" # Label for the legend\n",
    ")\n",
    "\n",
    "# Optional: Add labels and grid\n",
    "plt.xlabel(\"Natural Disaster Count\")\n",
    "plt.ylabel(\"Predicted Disease Increase Probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(os.path.join(FIGURES, f'fig06_{aim_variable}_PDP_{varname.upper()}_STD.jpg'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### knowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the grid boundaries based on the specified quantiles\n",
    "low_b = 0.0\n",
    "up_b = 11.0\n",
    "stripe = 1.0\n",
    "var = 'DisasterExpInd'\n",
    "\n",
    "# Generate the discrete grid of feature values\n",
    "potenital_values = np.arange(low_b, up_b, stripe)\n",
    "X_adjust = X.copy()\n",
    "\n",
    "pdp_list = []\n",
    "\n",
    "# Iterate through each model in the ensemble/list\n",
    "for model in model_list:\n",
    "    pdp = np.full_like(potenital_values, fill_value=np.nan)\n",
    "    \n",
    "    # Iterate through each grid point (potential value)\n",
    "    for idx, potenital_value in enumerate(potenital_values):\n",
    "        # 1. Substitute the feature column with the current fixed value\n",
    "        X_adjust[var] = potenital_value.astype(float)\n",
    "        X_adjust['HeardClimate_Dummy'] = 1.0\n",
    "        \n",
    "        # 2. Predict the outcome for the entire adjusted dataset\n",
    "        y_pred = model.predict_proba(X_adjust)[:,1]\n",
    "        \n",
    "        # 3. Calculate the partial dependence (average prediction)\n",
    "        pdp[idx] = np.mean(y_pred)\n",
    "    \n",
    "    pdp_list.append(pdp)\n",
    "\n",
    "pdp_array = np.array(pdp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_array_with_knowledge = pdp_array.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_mean = np.mean(pdp_array, axis = 0)\n",
    "pdp_std = np.std(pdp_array, axis = 0)\n",
    "\n",
    "# 1. Plot the mean PDP line (same as before)\n",
    "plt.plot(potenital_values, pdp_mean, linewidth=2, label=\"Mean Prediction\")\n",
    "\n",
    "# 2. Add the Error/Confidence Band using fill_between\n",
    "# The band represents [mean - std] to [mean + std]\n",
    "plt.fill_between(\n",
    "    potenital_values, \n",
    "    pdp_mean - pdp_std * 1.96,  # Lower bound\n",
    "    pdp_mean + pdp_std * 1.96,  # Upper bound\n",
    "    color='gray',        # Color of the shaded area\n",
    "    alpha=0.3,           # Transparency\n",
    "    label=\"$\\pm 1.96 \\sigma$\" # Label for the legend\n",
    ")\n",
    "\n",
    "# Optional: Add labels and grid\n",
    "plt.xlabel(\"Natural Disaster Count\")\n",
    "plt.ylabel(\"Predicted Disease Increase Probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "#plt.savefig(os.path.join(FIGURES, f'fig06_{aim_variable}_PDP_{varname.upper()}_STD.jpg'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### Plot togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(FIGURES := 'figures', exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp_mean_without_knowledge = np.mean(pdp_array_without_knowledge, axis = 0)\n",
    "pdp_std_without_knowledge = np.std(pdp_array_without_knowledge, axis = 0)\n",
    "pdp_mean_with_knowledge = np.mean(pdp_array_with_knowledge, axis = 0)\n",
    "pdp_std_with_knowledge = np.std(pdp_array_with_knowledge, axis = 0)\n",
    "\n",
    "# 1. Plot the mean PDP line (same as before)\n",
    "plt.plot(potenital_values, pdp_mean_without_knowledge, linewidth=2, label=\"Mean Prediction Without Knowledge\")\n",
    "plt.plot(potenital_values, pdp_mean_with_knowledge, linewidth=2, label=\"Mean Prediction With Knowledge\", color = 'red')\n",
    "\n",
    "\n",
    "# 2. Add the Error/Confidence Band using fill_between\n",
    "# The band represents [mean - std] to [mean + std]\n",
    "plt.fill_between(\n",
    "    potenital_values, \n",
    "    pdp_mean_without_knowledge - pdp_std_without_knowledge * 1.96,  # Lower bound\n",
    "    pdp_mean_without_knowledge + pdp_std_without_knowledge * 1.96,  # Upper bound\n",
    "    color='gray',        # Color of the shaded area\n",
    "    alpha=0.3,           # Transparency\n",
    "    label=\"$\\pm 1.96 \\sigma$\" # Label for the legend\n",
    ")\n",
    "plt.fill_between(\n",
    "    potenital_values, \n",
    "    pdp_mean_with_knowledge - pdp_std_with_knowledge * 1.96,  # Lower bound\n",
    "    pdp_mean_with_knowledge + pdp_std_with_knowledge * 1.96,  # Upper bound\n",
    "    color='gray',        # Color of the shaded area\n",
    "    alpha=0.3,           # Transparency\n",
    ")\n",
    "\n",
    "\n",
    "# Optional: Add labels and grid\n",
    "plt.xlabel(\"Natural Disaster Count\")\n",
    "plt.ylabel(\"Predicted Disease Increase Probability\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES, f'fig06_PDP.jpg'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join('results', 'pdp_array_without_knowledge.npy'), pdp_mean_without_knowledge)\n",
    "np.save(os.path.join('results', 'pdp_array_with_knowledge.npy'), pdp_array_with_knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
